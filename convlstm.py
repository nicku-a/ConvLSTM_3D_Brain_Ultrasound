# -*- coding: utf-8 -*-
"""ConvLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZrDdMETyWAV-LUDkjFPxks_oncUVn1vp
"""

import torch
from torch import Tensor
from torch import nn

class CLSTM_cell(nn.Module):

    def __init__(self, shape, input_channels, filter_size, num_features, timesteps):
        super(CLSTM_cell, self).__init__()

        self.shape = shape  # H, W
        self.input_channels = input_channels
        self.filter_size = filter_size
        self.num_features = num_features
        self.seq_len = timesteps
        self.padding = (filter_size - 1) // 2
        self.conv = nn.Sequential(
            nn.Conv2d(self.input_channels + self.num_features, 4 * self.num_features, self.filter_size, 1, self.padding),
            nn.GroupNorm(4 * self.num_features // 32, 4 * self.num_features))

    def forward(self, inputs=None, hidden_state=None):
        if hidden_state is None:
            hx = torch.zeros(inputs.size(1), self.num_features, self.shape[0], self.shape[1]).cuda()
            cx = torch.zeros(inputs.size(1), self.num_features, self.shape[0], self.shape[1]).cuda()
        else:
            hx, cx = hidden_state
        output_inner = []
        for index in range(self.seq_len):
            if inputs is None:
                x = torch.zeros(hx.size(0), self.input_channels, self.shape[0],  self.shape[1]).cuda()
            else:
                x = inputs[index, ...]

            combined = torch.cat((x, hx), 1)
            gates = self.conv(combined)  # gates: S, num_features*4, H, W
            # it should return 4 tensors: i,f,g,o
            ingate, forgetgate, cellgate, outgate = torch.split(gates, self.num_features, dim=1)
            ingate = torch.sigmoid(ingate)
            forgetgate = torch.sigmoid(forgetgate)
            cellgate = torch.tanh(cellgate)
            outgate = torch.sigmoid(outgate)

            cy = (forgetgate * cx) + (ingate * cellgate)
            hy = outgate * torch.tanh(cy)
            output_inner.append(hy)
            hx = hy
            cx = cy

        return torch.stack(output_inner), (hy, cy)


class ProcessOut(nn.Module):
    def __init__(self, n_f, ts, inputShape):

        super(ProcessOut, self).__init__()
        self.tanh = nn.Tanh()
        # self.drop = nn.Dropout2d(0.2)
        self.flat = nn.Flatten()
        self.linear = nn.Linear(n_f*ts*inputShape[0]*inputShape[1], 11)

    def forward(self, x):

        x = self.tanh(x)
        # x = self.drop(x)
        x = self.flat(x)
        y_pred = self.linear(x)
        return y_pred


class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x):
        return self.func(x)


class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()
        self.input_shape = (23,23)
        self.input_channels = 1
        self.filter_size = 3
        self.num_features1 = 32
        self.num_features2 = 32
        self.timesteps = 9

        # Unidirectional
        self.convlstm1 = CLSTM_cell(self.input_shape, self.input_channels, self.filter_size, self.num_features1, self.timesteps)
        self.convlstm2 = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        self.convlstm3 = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        self.convlstm4 = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        self.convlstm5 = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        self.convlstm6 = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        self.reshape_output = ProcessOut(self.num_features2, self.timesteps, self.input_shape)

        # Bidirectional
        # self.convlstm1FWD = CLSTM_cell(self.input_shape, self.input_channels, self.filter_size, self.num_features1, self.timesteps)
        # self.convlstm2FWD = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        # self.convlstm3FWD = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        # self.convlstm1INV = CLSTM_cell(self.input_shape, self.input_channels, self.filter_size, self.num_features1, self.timesteps)
        # self.convlstm2INV = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        # self.convlstm3INV = CLSTM_cell(self.input_shape, self.num_features1, self.filter_size, self.num_features2, self.timesteps)
        # self.reshape_output = ProcessOut(self.num_features2*2, self.timesteps, self.input_shape)
        
        self.lambda_1 = Lambda(lambda x: x.view(x.size(0), -1))

    def forward(self, x):
        
        x = x.transpose(0, 1)  # to S,B,1,23,23

        # Unidirectional
        x, (hy, cy) = self.convlstm1(x)
        x, (hy, cy) = self.convlstm2(x, (hy, cy))
        x, (hy, cy) = self.convlstm3(x, (hy, cy))
        x, (hy, cy) = self.convlstm4(x, (hy, cy))
        x, (hy, cy) = self.convlstm5(x, (hy, cy))
        x, (hy, cy) = self.convlstm6(x, (hy, cy))

        # # Bidirectional
        # xFWD, (hyFWD, cyFWD) = self.convlstm1FWD(x)
        # xINV, (hyINV, cyINV) = self.convlstm1INV(torch.flip(x, (0,)))
        # x = torch.cat((xFWD, xINV), dim=2)
        # xFWD, (hyFWD, cyFWD) = self.convlstm2FWD(x, (hyFWD, cyFWD))
        # xINV, (hyINV, cyINV) = self.convlstm2INV(torch.flip(x, (0,)), (hyINV, cyINV))
        # x = torch.cat((xFWD, xINV), dim=2)
        # xFWD, (hyFWD, cyFWD) = self.convlstm3FWD(x, (hyFWD, cyFWD))
        # xINV, (hyINV, cyINV) = self.convlstm3INV(torch.flip(x, (0,)), (hyINV, cyINV))
        # x = torch.cat((xFWD, xINV), dim=2)

        x = x.transpose(0, 1)  # to B,S,1,23,23
        reshaped_out = self.reshape_output(x)
        return self.lambda_1(reshaped_out)